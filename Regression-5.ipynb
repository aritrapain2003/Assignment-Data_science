{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea8e2c-5230-47db-be80-b1bec92c254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Elastic Net Regression and how does it differ from other regression techniques?\n",
    "\n",
    "\n",
    "\n",
    "Answer :\n",
    "    Elastic Net Regression is a statistical technique used in machine learning and statistics for regression analysis, which aims to overcome some of the limitations of traditional linear regression models, especially when dealing with datasets that have multicollinearity (high correlation between predictor variables) and a high number of predictors (features).\n",
    "\n",
    "Elastic Net Regression combines features of two other popular regression techniques: Ridge Regression and Lasso Regression.\n",
    "\n",
    "1. **Ridge Regression:** Ridge Regression adds a penalty term to the linear regression cost function that is proportional to the sum of squared coefficients (L2 regularization). This penalty term helps to prevent overfitting and can mitigate multicollinearity by shrinking the coefficients of correlated predictors.\n",
    "\n",
    "2. **Lasso Regression:** Lasso Regression also adds a penalty term to the linear regression cost function, but it uses the absolute values of the coefficients (L1 regularization). Lasso has the property of feature selection, meaning it can drive the coefficients of irrelevant or less important features to zero, effectively excluding them from the model.\n",
    "\n",
    "Elastic Net Regression combines both L2 and L1 regularization techniques to create a hybrid approach that inherits advantages from both Ridge and Lasso Regression while also addressing some of their limitations. The Elastic Net cost function includes both the L1 and L2 regularization terms, controlled by two hyperparameters: alpha and l1_ratio.\n",
    "\n",
    "- **Alpha (α):** It controls the overall strength of regularization. A higher alpha will result in stronger regularization, and as alpha approaches zero, Elastic Net becomes equivalent to linear regression.\n",
    "\n",
    "- **L1 Ratio (l1_ratio):** This parameter controls the balance between L1 and L2 regularization. When l1_ratio is 1, it's equivalent to Lasso Regression. When l1_ratio is 0, it's equivalent to Ridge Regression. When l1_ratio is between 0 and 1, it's a combination of both Lasso and Ridge penalties.\n",
    "\n",
    "In summary, Elastic Net Regression is a flexible regression technique that can handle multicollinearity, perform feature selection, and prevent overfitting. It's a good choice when you have a large number of predictors with potential collinearity issues, and you want to find a balance between Ridge and Lasso techniques.\n",
    "\n",
    "Compared to other regression techniques:\n",
    "\n",
    "- **Linear Regression:** Elastic Net includes regularization, making it more robust to overfitting and multicollinearity.\n",
    "  \n",
    "- **Ridge Regression:** Ridge focuses on reducing the impact of multicollinearity by shrinking coefficients, but it doesn't perform feature selection. Elastic Net can perform both tasks.\n",
    "\n",
    "- **Lasso Regression:** Lasso is great for feature selection, but when there are correlated predictors, it might arbitrarily select one and ignore others. Elastic Net's L2 regularization helps in handling this situation more effectively.\n",
    "\n",
    "- **Other advanced techniques:** Elastic Net can be especially useful when there is uncertainty about whether L1 or L2 regularization is more suitable, offering a balanced solution.\n",
    "\n",
    "Remember that the choice between these techniques depends on the specific characteristics of your dataset and your goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d486cce-a4c9-4505-a700-8e3c4f110fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the optimal values of the regularization parameters for Elastic Net Regression?\n",
    "\n",
    "\n",
    "Answer : Choosing the optimal values of the regularization parameters for Elastic Net Regression involves a process called hyperparameter tuning. The goal is to find the combination of the alpha (α) and l1_ratio parameters that results in the best performance of the model on your specific dataset. Here's a general approach to achieve this:\n",
    "\n",
    "1. **Grid Search or Random Search:** These are common methods for hyperparameter tuning. In grid search, you specify a set of possible values for alpha and l1_ratio, and the algorithm evaluates all possible combinations using techniques like cross-validation to determine which combination yields the best performance. In random search, you randomly sample combinations from the specified ranges.\n",
    "\n",
    "2. **Cross-Validation:** Cross-validation is crucial for assessing how well your model will generalize to new, unseen data. You typically use k-fold cross-validation, where you split your dataset into k subsets (folds), train the model on k-1 folds, and validate on the remaining fold. This process is repeated k times, rotating the validation fold each time. This helps to get a more reliable estimate of the model's performance for different parameter combinations.\n",
    "\n",
    "3. **Scoring Metric:** Choose an appropriate metric to evaluate the performance of your model during cross-validation. For regression tasks, metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or Mean Absolute Error (MAE) are commonly used.\n",
    "\n",
    "4. **Define Parameter Ranges:** Define the ranges for alpha and l1_ratio that you want to search over. For alpha, you can use a range of values from very small to relatively large. For l1_ratio, it's typically between 0 and 1 to cover the full range from Ridge to Lasso regularization. However, you can also try more focused ranges based on prior knowledge or experimentation.\n",
    "\n",
    "5. **Implement the Search:** Use the selected search method (grid search or random search) along with cross-validation to evaluate all the possible combinations of hyperparameters.\n",
    "\n",
    "6. **Evaluate Results:** After the search is complete, you'll have performance metrics for each combination of hyperparameters. Choose the combination that yields the best performance on the validation set.\n",
    "\n",
    "7. **Final Evaluation:** Once you have selected the optimal hyperparameters based on the validation set, it's a good practice to evaluate the model's performance on a separate test set that the model has not seen during training or validation. This gives you a final estimate of how well your model is likely to perform on new, unseen data.\n",
    "\n",
    "Keep in mind that hyperparameter tuning is an iterative process, and there might be interactions between hyperparameters that affect the model's performance. Also, the optimal values could vary depending on the specific characteristics of your dataset. It's essential to balance model complexity (lower alpha values) with the risk of overfitting (higher alpha values) and to find the right combination of regularization techniques to achieve the best predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9a1a0-d03d-4499-9dd6-8153c673554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What are the advantages and disadvantages of Elastic Net Regression?\n",
    "\n",
    "\n",
    "Answer :  \n",
    "    Elastic Net Regression offers a balanced approach between Ridge Regression and Lasso Regression, combining their advantages while addressing some of their limitations. Here are the advantages and disadvantages of Elastic Net Regression:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Handles Multicollinearity:** Elastic Net can handle datasets with multicollinearity (high correlation between predictor variables) better than traditional linear regression. The L2 regularization component helps in reducing the impact of correlated predictors.\n",
    "\n",
    "2. **Feature Selection:** Like Lasso Regression, Elastic Net can drive the coefficients of irrelevant or less important features to zero, effectively performing feature selection. This can simplify the model and enhance interpretability.\n",
    "\n",
    "3. **Flexibility:** The l1_ratio parameter allows you to control the balance between L1 and L2 regularization. This flexibility enables you to tailor the regularization method to your specific dataset's characteristics.\n",
    "\n",
    "4. **Overfitting Prevention:** Elastic Net helps in preventing overfitting by introducing regularization terms that penalize large coefficients. This is particularly beneficial when you have a high-dimensional dataset with a large number of features.\n",
    "\n",
    "5. **Suitable for Diverse Datasets:** Elastic Net can perform well on a wide range of datasets, including those with small sample sizes, noisy data, and moderate to high collinearity.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1. **Hyperparameter Tuning:** Like other regularization methods, Elastic Net requires tuning of hyperparameters (alpha and l1_ratio) to achieve optimal performance. This tuning process can be time-consuming and might not always result in the best possible parameters.\n",
    "\n",
    "2. **Interpretability:** While Elastic Net can help with feature selection, it might still include some features with small non-zero coefficients, which can make the model slightly less interpretable compared to Lasso Regression.\n",
    "\n",
    "3. **Data Scaling:** Elastic Net, like other regularization techniques, benefits from feature scaling. You might need to scale your features (e.g., using z-score normalization or min-max scaling) before applying Elastic Net to ensure consistent performance.\n",
    "\n",
    "4. **Limited for Large l1_ratio:** If the l1_ratio is set to a high value (close to 1), Elastic Net might perform similarly to Lasso Regression, which can lead to arbitrarily selecting one variable over others in the case of highly correlated predictors.\n",
    "\n",
    "5. **Complexity:** Elastic Net introduces additional complexity to the model due to its combined L1 and L2 regularization terms. While this complexity can be useful for handling specific data challenges, it can also make the model harder to understand and implement.\n",
    "\n",
    "In summary, Elastic Net Regression is a valuable tool when dealing with datasets that have multicollinearity and a high number of features. It strikes a balance between Ridge and Lasso Regression by offering both feature selection and coefficient shrinkage. However, it's important to consider the trade-offs and carefully tune the hyperparameters to achieve the best results for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e494c-a9b7-4689-89f2-948fafd47c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are some common use cases for Elastic Net Regression?\n",
    "\n",
    "\n",
    "Answer : \n",
    "    Elastic Net Regression is a statistical technique that combines the features of both Ridge Regression and Lasso Regression. It's particularly useful when dealing with high-dimensional data where there are more features than observations, and there is a potential for multicollinearity (correlation among predictor variables). Elastic Net addresses some of the limitations of Ridge and Lasso by introducing a hybrid regularization term that includes both L1 (Lasso) and L2 (Ridge) penalties. This makes Elastic Net versatile and suitable for various scenarios. Here are some common use cases:\n",
    "\n",
    "1. **Feature Selection and Dimensionality Reduction:** Elastic Net can effectively perform feature selection by shrinking some coefficients to zero (similar to Lasso) while also handling correlated predictors (unlike Lasso). This makes it useful for reducing the number of features in high-dimensional datasets.\n",
    "\n",
    "2. **Multicollinearity Handling:** When predictors are highly correlated, ordinary linear regression might result in unstable coefficient estimates. Elastic Net's combined L1 and L2 regularization helps mitigate multicollinearity issues, leading to more stable and interpretable models.\n",
    "\n",
    "3. **Regression with Lasso-like Sparsity and Ridge-like Stability:** Elastic Net combines the strengths of both Lasso and Ridge by introducing a balance between sparsity (some coefficients are set to zero) and stability (reduction of coefficient magnitudes). This is beneficial when both feature selection and controlling for multicollinearity are important.\n",
    "\n",
    "4. **Predictive Modeling:** Elastic Net can be used for predictive modeling tasks when you have a large number of features and limited data. It's commonly used in areas such as finance, economics, and biology, where datasets are often high-dimensional.\n",
    "\n",
    "5. **Regularized Regression in Machine Learning:** Elastic Net is used as a regularization technique in machine learning algorithms like linear regression, logistic regression, and support vector machines. It helps improve the generalization ability of models and prevents overfitting.\n",
    "\n",
    "6. **High-Dimensional Biological Data:** In genomics and other biological fields, researchers often encounter datasets with a large number of genes or biomarkers compared to the number of samples. Elastic Net can be useful for modeling relationships between these variables and outcomes.\n",
    "\n",
    "7. **Signal Processing and Image Analysis:** Elastic Net can be applied to signal processing tasks and image analysis when dealing with large amounts of data. It helps in selecting relevant features and reducing noise.\n",
    "\n",
    "8. **Text Analysis and Natural Language Processing:** In text analysis, where the number of features can be substantial, Elastic Net can assist in feature selection and building more robust models for tasks like sentiment analysis and text classification.\n",
    "\n",
    "9. **Collaborative Filtering and Recommendation Systems:** Elastic Net can be used to build recommendation systems that deal with high-dimensional user-item interaction data while handling issues like sparsity and multicollinearity.\n",
    "\n",
    "Overall, Elastic Net Regression is a powerful tool for balancing the trade-offs between feature selection, multicollinearity management, and predictive performance, making it suitable for a wide range of applications in data science, statistics, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ea5c8-44dc-4dd8-ae37-2cb87f56e1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How do you interpret the coefficients in Elastic Net Regression?\n",
    "\n",
    "\n",
    "Answer : \n",
    "    Interpreting coefficients in Elastic Net Regression is similar to interpreting coefficients in other regression techniques, but due to the combined L1 and L2 regularization of Elastic Net, there are some nuances to consider. Here's how you can interpret the coefficients in Elastic Net Regression:\n",
    "\n",
    "1. **Coefficient Sign:** Just like in regular linear regression, the sign of a coefficient indicates the direction of the relationship between the predictor variable and the response variable. A positive coefficient means that as the predictor variable increases, the response variable is expected to increase, and vice versa for a negative coefficient.\n",
    "\n",
    "2. **Coefficient Magnitude:** The magnitude of a coefficient indicates the strength of the relationship between the predictor variable and the response variable. Larger coefficients imply a more significant impact on the response variable. However, in Elastic Net, the magnitude of coefficients is influenced by both the L1 (Lasso) and L2 (Ridge) regularization terms.\n",
    "\n",
    "3. **Coefficient Significance:** To assess the statistical significance of a coefficient, you can look at the p-value associated with it. A low p-value (typically less than 0.05) suggests that the coefficient is statistically significant, meaning that the predictor variable is likely to have a non-zero effect on the response variable.\n",
    "\n",
    "4. **Coefficient Size and Regularization:** In Elastic Net, some coefficients might be exactly zero due to the L1 (Lasso) regularization. These coefficients correspond to features that the model considers irrelevant. Coefficients that are not exactly zero are influenced by both L1 and L2 penalties. The balance between these penalties determines the size of the coefficients. A higher L2 penalty tends to lead to smaller coefficients.\n",
    "\n",
    "5. **Coefficient Interaction and Multicollinearity:** Elastic Net can handle multicollinearity better than Lasso, but it's still important to be cautious when interpreting coefficients of correlated predictors. Coefficients might change in response to correlated variables being added or removed from the model, so their interpretations should be made in consideration of the overall context.\n",
    "\n",
    "6. **Coefficient Changes:** The coefficients in Elastic Net can change based on the choice of the hyperparameter \"alpha,\" which controls the balance between L1 and L2 regularization. When alpha is set to 1, Elastic Net becomes equivalent to Lasso, and when alpha is set to 0, it becomes equivalent to Ridge.\n",
    "\n",
    "7. **Scaling of Predictors:** Remember that the interpretation of coefficients also depends on the scaling of predictor variables. If predictors are on different scales, it might be helpful to standardize them before fitting the model to ensure fair comparison of coefficient magnitudes.\n",
    "\n",
    "In summary, interpreting coefficients in Elastic Net Regression involves considering the direction, magnitude, significance, regularization effects, and potential interactions with other predictors. The specific interpretation can vary based on the context of your data and the choices made during the modeling process, such as the value of the hyperparameter alpha and any preprocessing steps applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d6c56-82d7-4a40-bebb-b93a7fff6c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do you handle missing values when using Elastic Net Regression?\n",
    "\n",
    "\n",
    "Answer : \n",
    "    Handling missing values is an important step in any regression analysis, including Elastic Net Regression. Missing values can lead to biased or inefficient parameter estimates, so addressing them appropriately is crucial for obtaining accurate and reliable results. Here are several approaches you can consider when dealing with missing values in the context of Elastic Net Regression:\n",
    "\n",
    "1. **Remove Missing Data:** The simplest approach is to remove observations (rows) that have missing values for any of the predictor or response variables. However, this might lead to a reduction in the sample size and potentially biased results if missingness is not random.\n",
    "\n",
    "2. **Imputation Techniques:**\n",
    "   - **Mean or Median Imputation:** Replace missing values with the mean or median of the corresponding variable. While simple, this method doesn't consider the relationships between variables and might distort the relationships in the data.\n",
    "   - **Regression Imputation:** Predict the missing values using a regression model based on other variables with complete data. This can be done separately for each missing variable.\n",
    "   - **K-Nearest Neighbors (KNN) Imputation:** Impute missing values based on the values of the k-nearest neighbors in terms of other variables.\n",
    "   - **Multiple Imputation:** Generate multiple imputed datasets, each with slightly different imputed values, and analyze each dataset separately before combining the results. This accounts for the uncertainty introduced by imputation.\n",
    "\n",
    "3. **Advanced Imputation Methods:**\n",
    "   - **Predictive Mean Matching:** A variation of regression imputation that matches the imputed value to the nearest observed value.\n",
    "   - **Interpolation and Time-Series Methods:** For time-series data, techniques like linear interpolation or using historical data points can be helpful.\n",
    "   - **Model-Based Imputation:** Impute missing values using a predictive model (e.g., random forests, gradient boosting) based on other available variables.\n",
    "\n",
    "4. **Treating Missingness as a Predictor:** If missingness in a variable is informative and not completely at random, you can create an additional binary variable indicating whether the value is missing. This can capture potential relationships between the missingness pattern and the response.\n",
    "\n",
    "5. **Regularization Techniques:** In Elastic Net Regression, the regularization process can help mitigate the impact of missing values to some extent by shrinking the coefficients. However, imputing missing values before applying Elastic Net is still recommended for accurate results.\n",
    "\n",
    "6. **Domain Knowledge:** Depending on the context of your data, you might have domain-specific insights that guide the handling of missing values. For instance, if missing values are due to a specific reason, you might apply an appropriate imputation method.\n",
    "\n",
    "Remember that the choice of imputation method can influence your results, so it's important to carefully consider the characteristics of your data and the potential impact of different imputation strategies. Additionally, some software libraries and tools, like scikit-learn or specialized statistical packages, might offer built-in functions for imputation in combination with Elastic Net Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92137682-9573-4213-9cd9-27090134dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do you use Elastic Net Regression for feature selection?\n",
    "\n",
    "Answer : \n",
    "    Elastic Net Regression can be a powerful tool for feature selection due to its combined L1 (Lasso) and L2 (Ridge) regularization penalties. These penalties encourage sparsity in the coefficient estimates, effectively pushing some coefficients to exactly zero. Here's how you can use Elastic Net Regression for feature selection:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Clean your data by handling missing values and outliers appropriately.\n",
    "   - Standardize or normalize your predictor variables to ensure fair comparison of their coefficients, as Elastic Net is sensitive to variable scales.\n",
    "\n",
    "2. **Hyperparameter Tuning:**\n",
    "   - The key hyperparameter in Elastic Net is the mixing parameter \"alpha,\" which controls the balance between L1 (Lasso) and L2 (Ridge) regularization.\n",
    "   - When alpha is set to 1, Elastic Net behaves like Lasso regression, which strongly encourages sparsity.\n",
    "   - When alpha is set to 0, Elastic Net behaves like Ridge regression, which encourages coefficients to be small but does not push them to exactly zero.\n",
    "   - Typically, you'd perform a grid search or cross-validation to determine the optimal alpha value that best suits your data. A value between 0 and 1 is usually chosen.\n",
    "\n",
    "3. **Fit Elastic Net Model:**\n",
    "   - Fit an Elastic Net Regression model with the selected alpha value using your predictor variables and the response variable.\n",
    "   - The resulting coefficient estimates will provide insights into which features are deemed important by the model.\n",
    "\n",
    "4. **Feature Selection:**\n",
    "   - Examine the magnitude of the coefficient estimates. Coefficients that are close to zero or exactly zero are indicative of features that have been effectively selected by the model.\n",
    "   - Depending on the magnitude threshold you set (e.g., a small value like 0.001), you can consider features with coefficients above this threshold as selected features.\n",
    "\n",
    "5. **Interpretation and Further Analysis:**\n",
    "   - Analyze the selected features and their corresponding coefficients to understand their relationships with the response variable.\n",
    "   - Keep in mind that feature selection using Elastic Net is data-driven and can be influenced by the specific sample you're working with.\n",
    "\n",
    "6. **Model Evaluation:**\n",
    "   - Evaluate the performance of your selected feature model using appropriate metrics such as mean squared error (MSE) for regression tasks or accuracy for classification tasks.\n",
    "   - Consider using techniques like cross-validation to assess the model's generalization ability.\n",
    "\n",
    "It's important to note that Elastic Net automatically performs feature selection during the model fitting process by shrinking coefficients towards zero. The extent to which features are selected or retained depends on the alpha value. If you're primarily interested in feature selection, you can set a higher alpha to encourage more aggressive shrinking of coefficients. However, finding the right balance between feature selection and model performance is essential, as overly aggressive feature selection might lead to underfitting and decreased predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa2f4e-caab-4b1b-9c04-250bee1e89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickle is a standard module in Python that allows you to serialize (pickle) objects, including machine learning models, into a binary format. This serialized format can then be saved to a file or transferred over a network. Here's how you can pickle and unpickle a trained Elastic Net Regression model using the `pickle` module in Python:\n",
    "\n",
    "1. **Pickle a Trained Model:**\n",
    "\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Assuming you have a trained Elastic Net model named 'elastic_net_model'\n",
    "elastic_net_model = ElasticNet(alpha=0.5)  # Example, create a model\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "with open('elastic_net_model.pkl', 'wb') as file:\n",
    "    pickle.dump(elastic_net_model, file)\n",
    "\n",
    "\n",
    "In the above example, the `wb` mode is used to write the binary data to the file.\n",
    "\n",
    "2. **Unpickle a Trained Model:**\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Load the trained model from the pickle file\n",
    "with open('elastic_net_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "# Now you can use the loaded_model for prediction\n",
    "\n",
    "\n",
    "In the above example, the `rb` mode is used to read the binary data from the file.\n",
    "\n",
    "Keep in mind a few considerations when pickling and unpickling models:\n",
    "\n",
    "- **Model Compatibility:** When unpickling a model, make sure you are using the same version of the scikit-learn library (or whichever library you used to train the model) that was used when the model was pickled. Differences in library versions could lead to compatibility issues.\n",
    "\n",
    "- **Security:** Be cautious when unpickling models from untrusted sources. Pickle files can execute arbitrary code upon loading, which might pose a security risk. It's recommended to only unpickle models from trusted sources.\n",
    "\n",
    "- **Alternative Serialization:** Depending on your use case, you might consider using alternative serialization methods like joblib (`joblib.dump` and `joblib.load`) from the `joblib` library. It's more efficient for large data structures, including scikit-learn models.\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# To pickle a trained model\n",
    "joblib.dump(elastic_net_model, 'elastic_net_model.pkl')\n",
    "\n",
    "# To unpickle a trained model\n",
    "loaded_model = joblib.load('elastic_net_model.pkl')\n",
    "\n",
    "\n",
    "Remember to adjust the code to your specific model and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461a63f-13b7-4177-8b91-2bf94fcc233f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeacba0-24de-4364-88a3-82d9371b6501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f9dda-6d01-4e04-ab43-ec4245d8e6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
