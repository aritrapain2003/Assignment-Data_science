{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3410790-4fe8-4b42-8fbc-25e0318e1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results.\n",
    "\n",
    "\n",
    "\n",
    "Answer:\n",
    "    Analysis of Variance (ANOVA) is a statistical technique used to compare means among multiple groups, typically three or more. To use ANOVA effectively and ensure the validity of the results, certain assumptions need to be met. These assumptions include:\n",
    "\n",
    "1. **Independence**: Observations within each group are assumed to be independent of each other. This means that the value of an observation in one group should not be influenced by the value of an observation in another group.\n",
    "\n",
    "2. **Normality**: The data within each group should be approximately normally distributed. This assumption is more critical when sample sizes are small, as ANOVA becomes robust to violations of normality when sample sizes are large due to the Central Limit Theorem.\n",
    "\n",
    "3. **Homogeneity of Variance (Homoscedasticity)**: The variances of the groups should be roughly equal. In other words, the spread of the data within each group should be similar. Homoscedasticity ensures that the variation within groups is consistent across all groups.\n",
    "\n",
    "Now, let's discuss violations of these assumptions and how they can impact the validity of ANOVA results:\n",
    "\n",
    "1. **Independence Violation**: If observations are not independent, it can lead to pseudoreplication, where the assumption of having separate and distinct groups is violated. For example, if you measure the same subjects in multiple groups without proper accounting for dependencies, it can lead to inflated Type I error rates.\n",
    "\n",
    "2. **Normality Violation**: When the normality assumption is violated, the F-statistic used in ANOVA might not follow an F-distribution as assumed. This can affect the Type I error rate (false positive rate) and lead to incorrect conclusions. However, ANOVA is somewhat robust to mild departures from normality, especially with larger sample sizes.\n",
    "\n",
    "3. **Homoscedasticity Violation**: If the assumption of equal variances is violated, the F-statistic may be unreliable, and the Type I error rate can be affected. If the variances are not equal across groups, it can lead to unequal contributions of different groups to the F-statistic, potentially leading to false conclusions about group differences.\n",
    "\n",
    "It's worth noting that the impact of these violations can vary based on factors such as sample size, the degree of violation, and the specific type of ANOVA being used (e.g., one-way ANOVA, two-way ANOVA).\n",
    "\n",
    "When these assumptions are significantly violated, alternative approaches might be more appropriate. For example, non-parametric tests like the Kruskal-Wallis test can be used for group comparisons when the assumptions of ANOVA are not met.\n",
    "\n",
    "In practice, it's important to assess these assumptions before interpreting ANOVA results. This can involve techniques like visual inspection of data distributions, normality tests (e.g., Shapiro-Wilk test), and tests for homogeneity of variances (e.g., Levene's test). If assumptions are violated, careful consideration and potentially alternative methods should be employed to ensure the reliability of your statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340bbd2-2d65-4c1b-b68e-eff2578c0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the three types of ANOVA, and in what situations would each be used?\n",
    "\n",
    "Answer: \n",
    "\n",
    "There are three main types of Analysis of Variance (ANOVA) techniques, each designed to address different research questions and experimental designs:\n",
    "\n",
    "1. **One-Way ANOVA**:\n",
    "   - **Situation**: When you have one independent variable (factor) with three or more levels (groups) and you want to compare means across those groups.\n",
    "   - **Example**: An experiment that investigates the effect of different types of exercise (aerobic, strength training, flexibility) on cardiovascular fitness by measuring participants' heart rates after a certain period.\n",
    "\n",
    "2. **Two-Way ANOVA**:\n",
    "   - **Situation**: When you have two independent variables (factors) and you want to examine their individual and interaction effects on a dependent variable.\n",
    "   - **Example**: A study that examines how both gender and treatment type (e.g., medication, therapy) influence the reduction in symptoms of anxiety in patients.\n",
    "\n",
    "3. **Three-Way ANOVA**:\n",
    "   - **Situation**: When you have three independent variables (factors) and you want to study their individual and interaction effects on a dependent variable.\n",
    "   - **Example**: An agricultural study investigating the effects of three factors—fertilizer type, water availability, and sunlight exposure—on crop yield.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **One-Way ANOVA** is used when you have one independent variable and want to compare means across multiple groups.\n",
    "- **Two-Way ANOVA** is used when you have two independent variables and want to study their individual and combined effects on a dependent variable.\n",
    "- **Three-Way ANOVA** is used when you have three independent variables and want to examine their individual and combined effects on a dependent variable.\n",
    "\n",
    "Each type of ANOVA assesses the variation between and within groups or combinations of factors. The analysis allows you to determine whether the observed differences in means are statistically significant or if they could be due to random chance. If significant differences are found, further post hoc tests might be performed to identify which specific groups or combinations of factors differ from each other.\n",
    "\n",
    "It's important to ensure that the assumptions of ANOVA are met before interpreting the results. If the assumptions are not met, alternatives like non-parametric tests or transformations of data might be considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f497f477-4735-4193-bcb9-eb3bc9931434",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Answer:\n",
    "    The partitioning of variance in ANOVA refers to the breakdown of the total variance observed in a dataset into different components that can be attributed to various sources of variation. This breakdown helps to quantify the extent to which different factors or sources contribute to the observed variability in the data. Understanding the partitioning of variance is crucial because it allows researchers to determine the significance of these sources and make informed conclusions about the effects being studied.\n",
    "\n",
    "In ANOVA, the total variance of the data is divided into two main components:\n",
    "\n",
    "1. **Between-Group Variance (Systematic Variance)**: This component represents the variability between different groups or levels of the independent variable(s). It measures how much the means of the groups differ from each other. The larger the between-group variance relative to the within-group variance, the more likely it is that there is a significant effect of the independent variable on the dependent variable.\n",
    "\n",
    "2. **Within-Group Variance (Error Variance)**: This component represents the variability within each group or level of the independent variable(s). It accounts for the random variability or noise in the data that cannot be explained by the factors under study.\n",
    "\n",
    "The ANOVA process involves comparing the between-group variance to the within-group variance using an F-statistic. If the between-group variance is significantly larger than the within-group variance, it suggests that there are systematic differences between the groups, and the null hypothesis (no group differences) can be rejected.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "1. **Hypothesis Testing**: ANOVA helps researchers test hypotheses about the effects of different factors on the dependent variable. By partitioning the variance, researchers can determine if the observed differences are statistically significant or if they could have occurred by chance.\n",
    "\n",
    "2. **Identifying Important Factors**: By quantifying the contributions of different factors to the variance, researchers can identify which factors have the most significant influence on the dependent variable.\n",
    "\n",
    "3. **Experimental Design**: Partitioning variance can guide experimental design by showing which factors should be controlled or manipulated to minimize within-group variance and maximize between-group variance.\n",
    "\n",
    "4. **Interpretation**: Understanding the partitioning of variance helps researchers interpret the results of ANOVA correctly and make more informed conclusions about the relationships between variables.\n",
    "\n",
    "In summary, the partitioning of variance in ANOVA allows researchers to analyze the sources of variability in their data, assess the significance of effects, and draw meaningful conclusions about the relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c58939d9-048a-479a-9ad3-ed5c62e0419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 1110.4\n",
      "Explained Sum of Squares (SSE): 999.9999999999997\n",
      "Residual Sum of Squares (SSR): 110.40000000000043\n"
     ]
    }
   ],
   "source": [
    "# Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "# sum of squares (SSR) in a one-way ANOVA using Python?\n",
    "\n",
    "\n",
    "\n",
    "# In a one-way ANOVA, you can calculate the Total Sum of Squares (SST), Explained Sum of Squares (SSE), and Residual Sum of Squares (SSR) using Python. You would typically use a library like `numpy` to perform the calculations. Here's how you can do it step by step:\n",
    "\n",
    "# Let's assume you have a dataset with multiple groups and a corresponding array for each group's data points.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Simulated data for three groups\n",
    "group1 = np.array([10, 12, 14, 15, 18])\n",
    "group2 = np.array([20, 22, 24, 25, 28])\n",
    "group3 = np.array([30, 32, 34, 35, 38])\n",
    "\n",
    "# Combine all data into one array\n",
    "all_data = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate the Total Sum of Squares (SST)\n",
    "sst = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate the Explained Sum of Squares (SSE)\n",
    "group_means = [np.mean(group) for group in [group1, group2, group3]]\n",
    "sse = np.sum([len(group) * (mean - overall_mean)**2 for group, mean in zip([group1, group2, group3], group_means)])\n",
    "\n",
    "# Calculate the Residual Sum of Squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n",
    "\n",
    "\n",
    "# In this example, `sst` represents the total variability in the data, `sse` represents the variability explained by the group means, and `ssr` represents the unexplained variability, or error, in the data.\n",
    "\n",
    "# Keep in mind that in a real-world scenario, you might want to read the data from a file or database, and you may also want to conduct formal statistical analysis using libraries like `scipy.stats` to perform hypothesis testing and obtain p-values for ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35c9a07-4b9a-48dc-b060-fafaa4a47767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     sum_sq   df          F    PR(>F)\n",
      "FactorA          281.666667  1.0  15.552147  0.007593\n",
      "FactorB            0.066667  1.0   0.003681  0.953592\n",
      "FactorA:FactorB    0.066667  1.0   0.003681  0.953592\n",
      "Residual         108.666667  6.0        NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?\n",
    "\n",
    "\n",
    "\n",
    "# In a two-way ANOVA, you can calculate the main effects and interaction effects using Python. To perform this analysis, you'll need to use statistical libraries like `scipy.stats` or `statsmodels`. Here's an example using the `statsmodels` library:\n",
    "\n",
    "# Assuming you have a dataset with two factors (Factor A and Factor B) and a corresponding array of data points for each combination of factors:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Simulated data for a 2x2 factorial design\n",
    "data = {\n",
    "    'FactorA': np.repeat(['A1', 'A2'], 5),\n",
    "    'FactorB': np.tile(['B1', 'B2'], 5),\n",
    "    'Values': [10, 12, 15, 18, 20, 22, 24, 25, 28, 30]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit a two-way ANOVA model\n",
    "model = ols('Values ~ FactorA + FactorB + FactorA:FactorB', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n",
    "\n",
    "# In the ANOVA table produced by this code, you will see:\n",
    "\n",
    "# - **Main Effects**: Look at the rows for 'FactorA' and 'FactorB'. These represent the main effects of each factor. If the p-values for these effects are small (typically less than 0.05), it indicates that the corresponding factor has a significant effect on the dependent variable.\n",
    "\n",
    "# - **Interaction Effect**: Look at the row for 'FactorA:FactorB'. This represents the interaction effect between Factor A and Factor B. If the p-value for this interaction is small, it suggests that the interaction effect is significant. An interaction effect indicates that the combined effect of the two factors is not simply additive and that their influence on the dependent variable depends on each other.\n",
    "\n",
    "# Remember that a significant interaction effect might lead to additional analysis to understand the nature of the interaction, possibly involving post hoc tests or graphical exploration.\n",
    "\n",
    "# It's worth noting that interpreting interaction effects can be more complex than interpreting main effects. Understanding the context of the study and visualizing the data can aid in interpreting the results accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41ef4f-8103-48bb-b4b4-c9ce86084277",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Answer:\n",
    "    In the context of a one-way ANOVA, the F-statistic and its associated p-value are used to assess whether there are statistically significant differences in means between the groups. The F-statistic is calculated by comparing the ratio of variability between the groups to the variability within the groups. The p-value indicates the probability of obtaining such a result (or more extreme) if the null hypothesis were true.\n",
    "\n",
    "Given your information of an F-statistic of 5.23 and a p-value of 0.02, here's how you can interpret these results:\n",
    "\n",
    "1. **F-Statistic Value (5.23)**:\n",
    "   The F-statistic is a measure of how much the variability between the groups differs from the variability within the groups. A larger F-statistic indicates that the differences between group means are relatively larger compared to the random variability within each group.\n",
    "\n",
    "2. **P-Value (0.02)**:\n",
    "   The p-value associated with the F-statistic is 0.02. This p-value represents the probability of observing an F-statistic as extreme as 5.23 (or more extreme) under the assumption that there are no real differences between group means (null hypothesis).\n",
    "\n",
    "**Interpretation**:\n",
    "With a p-value of 0.02, the p-value is less than the commonly used significance level of 0.05. This suggests that you have enough evidence to reject the null hypothesis, which states that there are no significant differences between the group means.\n",
    "\n",
    "Therefore, you can conclude that there are statistically significant differences between at least some of the groups in your dataset. However, the ANOVA itself doesn't tell you which specific groups are different from each other—further post hoc tests or pairwise comparisons might be needed to determine which groups are driving this difference.\n",
    "\n",
    "Remember that statistical significance doesn't necessarily imply practical significance. It's important to consider the effect size and the context of the study to understand the practical implications of the observed differences between groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8560e-801b-41ff-a9d4-0301383ede15",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?\n",
    "\n",
    "\n",
    "\n",
    "Answer: \n",
    "    Handling missing data in a repeated measures ANOVA is essential for obtaining accurate and reliable results. Missing data can arise for various reasons, such as participant dropouts, technical errors, or incomplete responses. The way you handle missing data can impact the validity of your analysis and the conclusions you draw. Here are common methods for handling missing data in repeated measures ANOVA and their potential consequences:\n",
    "\n",
    "1. **Complete Case Analysis (Listwise Deletion)**:\n",
    "   In this approach, cases with any missing data in any variable are excluded from the analysis. This can lead to a reduction in sample size and potentially introduce bias if the missing data are not random. It can also decrease the power of the analysis, making it less likely to detect true effects.\n",
    "\n",
    "2. **Mean Imputation**:\n",
    "   Mean imputation involves replacing missing values with the mean of the observed values for that variable. While this is a simple approach, it can distort the variance and covariance structures in the data, leading to biased estimates and underestimated standard errors. It also doesn't account for the uncertainty introduced by imputation.\n",
    "\n",
    "3. **Last Observation Carried Forward (LOCF)**:\n",
    "   LOCF involves replacing missing values with the last observed value for that individual. This method assumes that the missing data pattern follows the trend of the last observed value. However, it might not accurately capture the true trajectory of the data and can lead to biased results.\n",
    "\n",
    "4. **Linear Interpolation**:\n",
    "   Linear interpolation estimates missing values based on the surrounding observed values. While it can provide more realistic estimates compared to mean imputation or LOCF, it assumes linear trends, which might not be appropriate for all types of data.\n",
    "\n",
    "5. **Multiple Imputation**:\n",
    "   Multiple imputation creates multiple plausible imputed datasets based on the observed data's distribution and correlation structure. Analyzing each imputed dataset separately and combining the results can provide more accurate parameter estimates, standard errors, and p-values. However, this approach can be computationally intensive and might require specialized software.\n",
    "\n",
    "6. **Model-Based Imputation**:\n",
    "   This involves fitting a model to the observed data and using the model to predict the missing values. While this approach can produce reasonable estimates when the model assumptions are met, it can introduce bias if the model is misspecified.\n",
    "\n",
    "The potential consequences of using different methods to handle missing data include bias, inaccurate estimates, incorrect p-values, and reduced power. It's important to choose a method that aligns with the underlying assumptions of your data and the research question. Furthermore, sensitivity analyses, which involve using different methods and comparing their impact on results, can help assess the robustness of your conclusions.\n",
    "\n",
    "Whichever method you choose, transparency about your approach and a discussion of the potential limitations of handling missing data are crucial when reporting the results of your repeated measures ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abb3e4-e933-45d7-aeb8-ff1010a2ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary.\n",
    "\n",
    "\n",
    "Answer:\n",
    "    Post-hoc tests are used after performing an Analysis of Variance (ANOVA) to make pairwise comparisons between groups when a significant overall difference has been found. These tests help determine which specific groups differ from each other. Some common post-hoc tests include:\n",
    "\n",
    "1. **Tukey's Honestly Significant Difference (HSD)**:\n",
    "   - **When to Use**: Tukey's HSD is a conservative method suitable for comparing all possible pairs of group means. It controls the familywise error rate, making it a good choice when you have a large number of pairwise comparisons.\n",
    "   - **Example**: In a study comparing the effects of three different diets on weight loss, a significant difference was found in the ANOVA. Tukey's HSD can be used to identify which specific diets resulted in significantly different weight loss.\n",
    "\n",
    "2. **Bonferroni Correction**:\n",
    "   - **When to Use**: The Bonferroni correction is a straightforward method that involves dividing the desired significance level by the number of comparisons to control the familywise error rate. It's suitable when you have a small number of pairwise comparisons.\n",
    "   - **Example**: In a study comparing the effectiveness of different teaching methods in three classrooms, a significant difference was found in the ANOVA. The Bonferroni correction can be used to adjust the significance level for each pairwise comparison.\n",
    "\n",
    "3. **LSD (Least Significant Difference)**:\n",
    "   - **When to Use**: LSD is less conservative than Tukey's HSD and is suitable when you have a small number of pairwise comparisons. It's important to note that using LSD without a significant omnibus test (like ANOVA) can inflate the Type I error rate.\n",
    "   - **Example**: In a clinical trial comparing three different medications for pain relief, a significant difference was found in the ANOVA. LSD can help identify which specific pairs of medications have significantly different effects on pain relief.\n",
    "\n",
    "4. **Dunnett's Test**:\n",
    "   - **When to Use**: Dunnett's test is used when you have one control group and want to compare the other groups to the control group. It's often used in situations where you're interested in whether treatments differ from a baseline or standard treatment.\n",
    "   - **Example**: In a study comparing the effectiveness of new medical treatments to a standard treatment, Dunnett's test can be used to determine if any of the new treatments have significantly different effects from the standard treatment.\n",
    "\n",
    "5. **Scheffe's Test**:\n",
    "   - **When to Use**: Scheffe's test is a more conservative post-hoc test that is suitable for cases with unequal group sizes and complex designs. It controls the familywise error rate but might result in wider confidence intervals compared to other methods.\n",
    "   - **Example**: In a psychological study with a complex factorial design involving multiple factors, Scheffe's test can be used to explore specific interactions between factors.\n",
    "\n",
    "It's important to select the appropriate post-hoc test based on the design of your study, the number of comparisons, and the level of control you want over the Type I error rate. Post-hoc tests help avoid making false conclusions about group differences and provide more specific insights into the relationships between groups in your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74571963-754b-4faf-a161-eb2c9ac18c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 0.4333013206515489\n",
      "p-value: 0.6492076183304043\n",
      "There are no significant differences between the mean weight loss of the three diets.\n"
     ]
    }
   ],
   "source": [
    "# Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "# 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "# to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "# Report the F-statistic and p-value, and interpret the results.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulated weight loss data for three diets (A, B, C)\n",
    "diet_a = np.array([1.5, 2.0, 1.8, 2.2, 2.5, 2.3, 1.7, 1.9, 2.1, 1.8,\n",
    "                   2.4, 2.6, 2.0, 1.6, 1.9, 2.2, 1.7, 1.8, 2.3, 1.5,\n",
    "                   2.1, 2.0, 1.9, 1.8, 1.7, 1.6, 2.2, 2.1, 1.8, 1.9,\n",
    "                   2.0, 2.3, 1.5, 2.1, 1.8, 2.2, 2.5, 2.3, 1.7, 1.9,\n",
    "                   2.4, 2.6, 2.0, 1.6, 1.9, 2.2, 1.7, 1.8, 2.3, 1.5])\n",
    "\n",
    "diet_b = np.array([1.2, 1.5, 1.8, 2.1, 1.7, 1.9, 2.2, 1.5, 2.0, 2.3,\n",
    "                   1.6, 1.9, 2.4, 2.6, 2.0, 1.6, 1.9, 2.2, 1.7, 1.8,\n",
    "                   2.3, 1.5, 2.1, 2.0, 1.9, 1.8, 1.7, 2.2, 2.1, 1.8,\n",
    "                   1.9, 2.0, 2.3, 1.5, 2.1, 1.8, 2.2, 1.7, 1.9, 2.4,\n",
    "                   2.6, 2.0, 1.6, 1.9, 2.2, 1.7, 1.8, 2.3, 1.5])\n",
    "\n",
    "diet_c = np.array([2.0, 2.3, 1.7, 1.9, 2.1, 1.8, 2.2, 2.5, 2.3, 1.7,\n",
    "                   1.8, 2.3, 1.5, 2.1, 2.0, 1.9, 1.8, 1.7, 1.6, 2.2,\n",
    "                   2.1, 1.8, 1.9, 2.0, 2.3, 1.5, 2.1, 1.8, 2.2, 1.7,\n",
    "                   1.9, 2.4, 2.6, 2.0, 1.6, 1.9, 2.2, 1.7, 1.8, 2.3,\n",
    "                   1.5, 2.0, 1.8, 2.1, 1.7, 1.9, 2.2, 2.5])\n",
    "\n",
    "# Combine all data into one array\n",
    "all_data = np.concatenate([diet_a, diet_b, diet_c])\n",
    "\n",
    "# Group labels for each diet\n",
    "group_labels = ['A'] * len(diet_a) + ['B'] * len(diet_b) + ['C'] * len(diet_c)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_a, diet_b, diet_c)\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There are significant differences between the mean weight loss of the three diets.\")\n",
    "else:\n",
    "    print(\"There are no significant differences between the mean weight loss of the three diets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d07be25-66ac-4ee6-8c4a-58e4e9334ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         sum_sq    df         F    PR(>F)\n",
      "Software               3.896300   2.0  0.237096  0.789442\n",
      "Experience             0.042025   1.0  0.005115  0.943157\n",
      "Software:Experience    0.358944   2.0  0.021842  0.978400\n",
      "Residual             690.203193  84.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "# complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "# randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "# complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "# interaction effects between the software programs and employee experience level (novice vs.\n",
    "# experienced). Report the F-statistics and p-values, and interpret the results.\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Simulated data for the task completion time with factors: Software, Experience\n",
    "data = {\n",
    "    'Software': np.repeat(['A', 'B', 'C'], 30),\n",
    "    'Experience': np.tile(['Novice', 'Experienced'], 45),\n",
    "    'Time': np.random.normal(loc=15, scale=3, size=90)  # Simulated time data\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit a two-way ANOVA model with interaction\n",
    "model = ols('Time ~ Software * Experience', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In this example, Software and Experience are treated as categorical variables. The Software variable has three levels (A, B, C), and the Experience variable has two levels (Novice, Experienced). The simulated Time data represents the time taken to complete the task.\n",
    "\n",
    "# The Software * Experience term in the formula includes both main effects and the interaction effect in the ANOVA model. The typ=2 argument in the anova_lm function specifies that a Type 2 ANOVA is used.\n",
    "\n",
    "# The output of the anova_table will provide you with F-statistics and p-values for the main effects of Software and Experience, as well as the interaction effect.\n",
    "\n",
    "# Interpretation:\n",
    "\n",
    "# Check the p-values for the main effects of Software and Experience. If any p-value is below your chosen significance level (e.g., 0.05), you would conclude that there is a significant main effect for that factor.\n",
    "\n",
    "# Look at the p-value for the interaction effect (Software * Experience). If this p-value is below your significance level, it suggests that there is a significant interaction effect. This means that the effect of one factor on the outcome variable depends on the level of the other factor.\n",
    "\n",
    "# Remember that significant interactions can complicate the interpretation of main effects. Further post hoc analyses or graphical exploration might be needed to understand the nature of the interaction and the direction of the effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f04d50da-5f9c-4787-9e3b-477a184ef501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test results:\n",
      "t-statistic: -6.766067958788337\n",
      "p-value: 9.722492183014506e-10\n",
      "There is a significant difference in test scores between the control and experimental groups.\n",
      "\n",
      "Mann-Whitney U test results:\n",
      "Mann-Whitney U statistic: 444.0\n",
      "p-value: 2.6858883224271995e-08\n",
      "There is a significant difference between the groups based on the Mann-Whitney U test.\n"
     ]
    }
   ],
   "source": [
    "# Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "# scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "# experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "# two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "# between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "# group(s) differ significantly from each other.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulated test scores for the control (group 0) and experimental (group 1) groups\n",
    "control_scores = np.array([70, 75, 80, 65, 72, 78, 82, 68, 74, 76,\n",
    "                           85, 71, 79, 68, 75, 70, 73, 81, 79, 67,\n",
    "                           72, 77, 76, 82, 74, 70, 78, 75, 83, 68,\n",
    "                           71, 72, 69, 80, 74, 76, 70, 81, 75, 68,\n",
    "                           73, 77, 79, 67, 72, 78, 75, 82, 69, 73])\n",
    "\n",
    "experimental_scores = np.array([76, 82, 88, 72, 79, 85, 90, 74, 81, 84,\n",
    "                                93, 78, 86, 75, 82, 77, 80, 87, 85, 73,\n",
    "                                78, 83, 82, 88, 81, 77, 85, 82, 89, 74,\n",
    "                                77, 78, 76, 87, 81, 83, 77, 88, 82, 75,\n",
    "                                80, 84, 86, 73, 78, 85, 82, 89, 76, 80])\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "print(\"Two-sample t-test results:\")\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the t-test results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There is a significant difference in test scores between the control and experimental groups.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the control and experimental groups.\")\n",
    "\n",
    "# Follow up with post-hoc test (e.g., Mann-Whitney U test)\n",
    "if p_value < alpha:\n",
    "    mwu_statistic, mwu_p_value = stats.mannwhitneyu(control_scores, experimental_scores)\n",
    "    print(\"\\nMann-Whitney U test results:\")\n",
    "    print(\"Mann-Whitney U statistic:\", mwu_statistic)\n",
    "    print(\"p-value:\", mwu_p_value)\n",
    "    if mwu_p_value < alpha:\n",
    "        print(\"There is a significant difference between the groups based on the Mann-Whitney U test.\")\n",
    "    else:\n",
    "        print(\"There is no significant difference between the groups based on the Mann-Whitney U test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e34b9-1db3-44e1-b68d-816189c0420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "# retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "# on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "\n",
    "# significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "# hoc test to determine which store(s) differ significantly from each other.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
