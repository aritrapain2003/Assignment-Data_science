{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0bb68e-c76c-4c39-b743-774e6cf63daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a projection and how is it used in PCA?\n",
    "Ans :\n",
    "    A projection, in the context of Principal Component Analysis (PCA), refers to the transformation of data points from their original high-dimensional space into a lower-dimensional subspace. PCA is a dimensionality reduction technique used to capture the most important information or variability in a dataset while reducing its dimensionality.\n",
    "\n",
    "Here's how projection is used in PCA:\n",
    "\n",
    "1. **Centering the Data:** The first step in PCA is to center the data by subtracting the mean (average) value of each feature from all data points. This step ensures that the data is centered at the origin of the high-dimensional space.\n",
    "\n",
    "2. **Covariance Matrix Calculation:** PCA then calculates the covariance matrix of the centered data. The covariance matrix represents the relationships between pairs of features and helps determine how they vary together.\n",
    "\n",
    "3. **Eigendecomposition:** The next step is to perform an eigendecomposition (eigenvalue decomposition) of the covariance matrix. This decomposition yields a set of eigenvectors and corresponding eigenvalues. The eigenvectors represent the principal components of the data, and the eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. **Selecting Principal Components:** PCA allows you to select a subset of the principal components (eigenvectors) based on the amount of variance you want to retain in the reduced-dimensional representation. Typically, you rank the eigenvalues in descending order and choose the top k eigenvectors, where k is the desired dimensionality of the reduced space.\n",
    "\n",
    "5. **Projection:** Finally, you project the original data points onto the selected principal components. This projection involves taking a linear combination of the original features with the coefficients given by the selected eigenvectors. The result is a set of new feature values in the lower-dimensional subspace.\n",
    "\n",
    "The key idea in PCA is that the first principal component captures the most variance in the data, the second principal component captures the second most, and so on. By selecting a subset of these principal components (typically the top k), you create a lower-dimensional representation of the data that retains as much of the original variance as possible. This reduced-dimensional representation can be used for visualization, data compression, or as input for other machine learning algorithms.\n",
    "\n",
    "In summary, projection in PCA involves transforming data from a high-dimensional space into a lower-dimensional space while preserving as much of the variance as possible. It is achieved by selecting a subset of principal components and projecting the data onto these components, resulting in a reduced-dimensional representation of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6eef9-d8fb-468f-adae-e80ec4171a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How does the optimization problem in PCA work, and what is it trying to achieve?\n",
    "Ans :\n",
    "    The optimization problem in Principal Component Analysis (PCA) aims to find the principal components (eigenvectors) of a dataset's covariance matrix while maximizing the variance explained by these components. In PCA, the goal is to reduce the dimensionality of the data while retaining as much of the original variance as possible. Here's how the optimization problem in PCA works and what it is trying to achieve:\n",
    "\n",
    "1. **Data Centering:** The first step in PCA is to center the data by subtracting the mean (average) value of each feature from all data points. This step ensures that the data is centered at the origin of the high-dimensional space.\n",
    "\n",
    "2. **Covariance Matrix:** PCA calculates the covariance matrix of the centered data. The covariance matrix represents the relationships between pairs of features and how they vary together. It is a square matrix where each element (i, j) represents the covariance between feature i and feature j.\n",
    "\n",
    "3. **Eigenvalue Decomposition:** The central optimization problem in PCA involves finding the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions (principal components) in which the data varies the most, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "   - The optimization problem can be stated as follows: For a given covariance matrix C, find a set of unit-length vectors (eigenvectors) v1, v2, ..., vk, and their corresponding eigenvalues λ1, λ2, ..., λk, such that the variance of the data when projected onto these vectors is maximized.\n",
    "\n",
    "   - Mathematically, this problem can be expressed as: Maximize the expression ∑i=1 to k λi, subject to the constraint that the eigenvectors v1, v2, ..., vk are orthogonal to each other and have unit length (i.e., ||vi|| = 1 for all i).\n",
    "\n",
    "4. **Selecting Principal Components:** After solving the eigenvalue decomposition, you obtain the eigenvectors and eigenvalues. The eigenvalues are typically ranked in descending order, and the corresponding eigenvectors represent the principal components. The first principal component (eigenvector with the largest eigenvalue) explains the most variance, the second principal component explains the second most, and so on.\n",
    "\n",
    "5. **Reduced-Dimensional Representation:** By selecting a subset of the top-k principal components (eigenvectors), you can project the original data onto this lower-dimensional subspace. The reduced-dimensional representation retains the most important information in the data while reducing its dimensionality.\n",
    "\n",
    "The optimization problem in PCA is trying to achieve the following:\n",
    "\n",
    "- Maximize Variance Retention: It seeks to find a set of orthogonal vectors (principal components) that maximize the variance explained by projecting the data onto these components. By selecting the top-k principal components, you aim to retain as much of the original variance as possible.\n",
    "\n",
    "- Dimensionality Reduction: The optimization problem aims to reduce the dimensionality of the data by selecting a subset of principal components. This reduction in dimensionality simplifies data analysis, visualization, and model training while preserving the most essential information.\n",
    "\n",
    "In summary, PCA's optimization problem aims to find the principal components of a dataset's covariance matrix to maximize the variance explained by these components. By solving this problem, PCA provides a reduced-dimensional representation of the data that retains the most important information while reducing dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300b5f6-235a-432b-bfd1-0a8820b31963",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the relationship between covariance matrices and PCA?\n",
    "Ans :\n",
    "    Covariance matrices play a central role in Principal Component Analysis (PCA). PCA is a dimensionality reduction technique used to find the principal components of a dataset by analyzing the covariance structure of the data. The relationship between covariance matrices and PCA can be understood as follows:\n",
    "\n",
    "1. **Covariance Matrix Calculation:**\n",
    "   \n",
    "   - In PCA, the first step is to calculate the covariance matrix of the dataset. The covariance matrix, denoted as C, is a square matrix where each element C[i][j] represents the covariance between feature i and feature j.\n",
    "\n",
    "   - The covariance between two variables measures how they vary together. A positive covariance indicates that the variables tend to increase or decrease together, while a negative covariance indicates that one tends to increase when the other decreases.\n",
    "\n",
    "   - Mathematically, the covariance between two variables X and Y is calculated as follows:\n",
    "\n",
    "     \\[Cov(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})\\]\n",
    "\n",
    "   - Here, n is the number of data samples, \\(X_i\\) and \\(Y_i\\) are individual data points, and \\(\\bar{X}\\) and \\(\\bar{Y}\\) are the means of X and Y, respectively.\n",
    "\n",
    "2. **Eigenvalue Decomposition of Covariance Matrix:**\n",
    "\n",
    "   - After calculating the covariance matrix C, the next step in PCA involves performing an eigenvalue decomposition (eigendecomposition) of this matrix.\n",
    "\n",
    "   - Eigendecomposition yields a set of eigenvectors and corresponding eigenvalues. Each eigenvector represents a principal component, and each eigenvalue represents the amount of variance explained by the corresponding principal component.\n",
    "\n",
    "   - The eigenvectors of the covariance matrix C are the directions in the original feature space along which the data varies the most. These eigenvectors are typically referred to as the principal components of the data.\n",
    "\n",
    "3. **Principal Component Selection:**\n",
    "\n",
    "   - Once you have obtained the eigenvectors and eigenvalues, you can rank the eigenvalues in descending order. The eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "   - By selecting the top-k eigenvectors (where k is the desired dimensionality of the reduced space), you are choosing the most significant principal components that capture the most variance in the data.\n",
    "\n",
    "4. **Projection:**\n",
    "\n",
    "   - The final step in PCA is to project the original data onto the selected principal components. This projection involves taking a linear combination of the original features with the coefficients given by the selected eigenvectors.\n",
    "\n",
    "   - The result is a reduced-dimensional representation of the data in a subspace spanned by the chosen principal components.\n",
    "\n",
    "In summary, the relationship between covariance matrices and PCA is that the covariance matrix represents the relationships and variances between features in the dataset. PCA leverages this covariance matrix to find the principal components, which are orthogonal directions in the original feature space that capture the most variance in the data. The covariance matrix and its eigenvectors and eigenvalues are fundamental to the dimensionality reduction process in PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25f002-1d70-4400-9092-bb70ddab11f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the choice of number of principal components impact the performance of PCA?\n",
    "Ans :\n",
    "    The choice of the number of principal components in PCA has a significant impact on the performance and effectiveness of PCA for various data analysis and machine learning tasks. It influences several aspects of PCA, and the optimal number of principal components depends on the specific goals and characteristics of your dataset. Here's how the choice of the number of principal components impacts PCA's performance:\n",
    "\n",
    "1. **Variance Explained:**\n",
    "\n",
    "   - The number of principal components determines how much of the total variance in the data is retained in the reduced-dimensional representation. Selecting more principal components preserves more variance but may lead to a higher-dimensional representation.\n",
    "\n",
    "   - By choosing a smaller number of principal components, you reduce the dimensionality of the data but may sacrifice some of the variance. The trade-off is between dimensionality reduction and variance retention.\n",
    "\n",
    "2. **Dimensionality Reduction:**\n",
    "\n",
    "   - If you select a smaller number of principal components, you achieve a more aggressive dimensionality reduction, which can simplify data analysis, visualization, and modeling.\n",
    "\n",
    "   - Conversely, choosing a larger number of principal components results in a less aggressive reduction, which may be beneficial when you want to retain more fine-grained details or when you're working with data where most of the features are important.\n",
    "\n",
    "3. **Interpretability:**\n",
    "\n",
    "   - A smaller number of principal components often leads to a more interpretable reduced-dimensional representation, as it retains only the most important directions in the data.\n",
    "\n",
    "   - In contrast, a larger number of principal components can make interpretation more challenging, as it introduces additional dimensions that may not have clear semantic meaning.\n",
    "\n",
    "4. **Computational Efficiency:**\n",
    "\n",
    "   - Fewer principal components require less computational resources for both training and inference in machine learning models. This can be essential in scenarios with limited computational capacity.\n",
    "\n",
    "   - A larger number of principal components can lead to increased computational demands.\n",
    "\n",
    "5. **Overfitting and Noise:**\n",
    "\n",
    "   - Selecting too many principal components can risk overfitting, especially if the dataset contains noise or if the selected components capture random variations in the data.\n",
    "\n",
    "   - A smaller number of principal components may provide a cleaner representation of the underlying patterns, reducing the risk of overfitting.\n",
    "\n",
    "6. **Visualization:**\n",
    "\n",
    "   - In visualization tasks, choosing the number of principal components can affect the quality and informativeness of the visual representation. Fewer components may yield clearer visualizations, while more components may capture finer details.\n",
    "\n",
    "7. **Model Performance:**\n",
    "\n",
    "   - The choice of the number of principal components can impact the performance of downstream machine learning models. Too few components may lead to underfitting, while too many may lead to overfitting.\n",
    "\n",
    "   - Cross-validation and model evaluation can help you determine the optimal number of components for your specific modeling task.\n",
    "\n",
    "To make an informed decision about the number of principal components, you can use techniques like cumulative explained variance plots, scree plots, cross-validation, and domain knowledge. These methods can help you strike the right balance between dimensionality reduction and variance retention, aligning PCA with your specific goals and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76533efe-e610-40f2-bdee-3a8d87544e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?\n",
    "\n",
    "Ans :\n",
    "    PCA can be used as a feature selection technique, although it is technically a dimensionality reduction method. When employed for feature selection, PCA serves the purpose of selecting a subset of the original features based on their importance in capturing the variance in the data. This process can be beneficial in various ways:\n",
    "\n",
    "1. **Variance-Based Selection:** PCA ranks the original features based on their contribution to the overall variance in the data. Features with high variance are considered more informative, while those with low variance may be less relevant. By selecting the top-ranked features, you can focus on the most informative ones while discarding less important ones.\n",
    "\n",
    "2. **Dimensionality Reduction:** PCA inherently reduces the dimensionality of the data by selecting a smaller number of principal components (a linear combination of original features). These principal components capture the most essential information in the data, which can be seen as a form of feature selection. The reduced feature set can simplify modeling and improve computational efficiency.\n",
    "\n",
    "3. **Multicollinearity Mitigation:** PCA can help mitigate multicollinearity, a situation where two or more features are highly correlated. High multicollinearity can lead to numerical instability in some machine learning algorithms. PCA transforms the correlated features into orthogonal (uncorrelated) principal components, reducing multicollinearity in the reduced-dimensional representation.\n",
    "\n",
    "4. **Noise Reduction:** PCA can also reduce the impact of noise in the data. By focusing on the principal components that capture the most variance, PCA tends to emphasize the underlying patterns in the data and diminish the influence of noise.\n",
    "\n",
    "5. **Model Generalization:** A reduced feature set obtained through PCA may lead to better model generalization. With fewer features, models are less likely to overfit the training data, resulting in improved performance on unseen data.\n",
    "\n",
    "6. **Visualization:** PCA can be used for data visualization, especially when reducing high-dimensional data to two or three dimensions. Selecting the most relevant principal components for visualization purposes can provide insights into data patterns and relationships among data points.\n",
    "\n",
    "7. **Data Compression:** In scenarios where storage or memory constraints are important, using PCA as feature selection can lead to significant data compression. The reduced-dimensional data representation requires less memory and storage space.\n",
    "\n",
    "To use PCA for feature selection:\n",
    "\n",
    "1. **Standardize the Data:** Ensure that the data is standardized (mean-centered and scaled) so that all features have a similar scale. This is crucial for PCA, as it is sensitive to the relative scales of features.\n",
    "\n",
    "2. **Perform PCA:** Apply PCA to the standardized data to calculate the principal components and their associated eigenvalues.\n",
    "\n",
    "3. **Rank Features:** Rank the original features based on the magnitude of their contributions to the principal components. Features associated with higher eigenvalues are considered more important.\n",
    "\n",
    "4. **Select Features:** Choose the top-ranked features according to your criteria. This may involve selecting a fixed number of features, specifying a threshold for eigenvalues, or using cumulative explained variance to determine the number of features to retain.\n",
    "\n",
    "5. **Use Selected Features:** Utilize the selected features for subsequent data analysis or machine learning tasks.\n",
    "\n",
    "It's important to note that while PCA-based feature selection can be effective in certain scenarios, it may not always be the best choice, particularly if the goal is to preserve the interpretability of the features or if there are non-linear relationships in the data that PCA may not capture well. Careful consideration of the problem and the specific objectives is necessary when deciding whether to use PCA for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc4af9-d84f-44a9-aeb5-b62f7468f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some common applications of PCA in data science and machine learning?\n",
    "Ans: \n",
    "    Principal Component Analysis (PCA) is a versatile technique with a wide range of applications in data science and machine learning. Its ability to reduce the dimensionality of data while preserving important information makes it valuable in various domains. Here are some common applications of PCA:\n",
    "\n",
    "1. **Dimensionality Reduction:** PCA's primary use is in dimensionality reduction. It is applied to high-dimensional data to reduce the number of features while retaining as much relevant information as possible. This can simplify data analysis, visualization, and modeling.\n",
    "\n",
    "2. **Data Visualization:** PCA is often used for data visualization when dealing with high-dimensional datasets. By projecting data onto a lower-dimensional subspace (e.g., 2D or 3D), it allows for easier visualization and exploration of data patterns and relationships.\n",
    "\n",
    "3. **Image Compression:** In image processing, PCA can be applied to reduce the dimensionality of images while preserving image quality. This is useful for image compression, storage, and transmission in applications like multimedia and video streaming.\n",
    "\n",
    "4. **Face Recognition:** PCA is commonly used in facial recognition systems. It can help extract important facial features and reduce the dimensionality of face images, making it easier to compare and recognize faces.\n",
    "\n",
    "5. **Speech Recognition:** In speech processing, PCA can be applied to extract relevant features from audio signals, reducing the computational complexity of speech recognition systems and improving their accuracy.\n",
    "\n",
    "6. **Anomaly Detection:** PCA is used in anomaly detection to identify unusual patterns or outliers in data. By reducing the dimensionality of the data, it can help identify deviations from normal behavior.\n",
    "\n",
    "7. **Biological Data Analysis:** In genomics and bioinformatics, PCA is applied to analyze high-dimensional biological data, such as gene expression profiles or DNA microarray data. It can help identify patterns and relationships among genes or samples.\n",
    "\n",
    "8. **Financial Analysis:** PCA is used in finance for risk assessment, portfolio optimization, and asset pricing. It can help identify underlying factors or commonalities among financial assets and reduce the risk of multicollinearity in financial models.\n",
    "\n",
    "9. **Customer Segmentation:** In marketing and customer analytics, PCA can assist in customer segmentation by reducing the dimensionality of customer data and identifying groups of customers with similar behavior or preferences.\n",
    "\n",
    "10. **Recommendation Systems:** PCA can be applied to collaborative filtering-based recommendation systems to reduce the dimensionality of user-item interaction data, making it computationally more efficient and improving recommendation quality.\n",
    "\n",
    "11. **Chemometrics:** In chemistry and spectroscopy, PCA is used for feature extraction and data compression in the analysis of complex spectra or chemical compositions.\n",
    "\n",
    "12. **Natural Language Processing (NLP):** PCA can be applied to text data, such as document-term matrices or word embeddings, to reduce the dimensionality of textual features and improve the efficiency of NLP models.\n",
    "\n",
    "13. **Quality Control:** In manufacturing and quality control, PCA can be used to monitor and control processes by reducing the dimensionality of sensor data and identifying deviations from the expected patterns.\n",
    "\n",
    "14. **Geospatial Data Analysis:** PCA is employed in geospatial data analysis to reduce the dimensionality of spatial data, allowing for better visualization and analysis of geographic patterns.\n",
    "\n",
    "These are just a few examples of the many applications of PCA in data science and machine learning. PCA's ability to simplify complex data, reduce computational complexity, and extract meaningful patterns makes it a valuable tool in various domains where high-dimensional data is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee1712-6236-402e-b963-03d04a53f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7.What is the relationship between spread and variance in PCA?\n",
    "Ans : \n",
    "    In Principal Component Analysis (PCA), spread and variance are closely related concepts that refer to the distribution of data along the principal components. Understanding this relationship is essential for interpreting PCA results and assessing the importance of principal components. Here's how spread and variance are connected in PCA:\n",
    "\n",
    "1. **Spread:**\n",
    "   \n",
    "   - Spread, in the context of PCA, refers to how the data points are distributed along the principal components. It is a measure of how much variation or dispersion exists along each principal component's direction.\n",
    "   \n",
    "   - When data points are more spread out along a particular principal component, it means that the data varies more in that direction. Conversely, when data points are closely clustered along a principal component, there is less spread in that direction.\n",
    "\n",
    "2. **Variance:**\n",
    "\n",
    "   - Variance is a statistical measure that quantifies the amount of variation or dispersion in a dataset. In PCA, variance is used to evaluate the importance of each principal component.\n",
    "\n",
    "   - The variance of a principal component represents how much of the total variance in the data is captured by that specific component. Principal components are ranked in terms of the amount of variance they explain, with the first principal component capturing the most variance, the second capturing the second most, and so on.\n",
    "\n",
    "The relationship between spread and variance in PCA can be summarized as follows:\n",
    "\n",
    "- The spread of data points along a principal component is directly related to the variance explained by that principal component.\n",
    "\n",
    "- A principal component with high spread corresponds to a high variance, indicating that it captures a significant amount of the overall variation in the data.\n",
    "\n",
    "- Conversely, a principal component with low spread corresponds to a low variance, indicating that it captures relatively little variation in the data.\n",
    "\n",
    "- When performing PCA, you often analyze the variance explained by each principal component to determine their relative importance. Principal components with higher variances are considered more informative and may be selected to represent the data in a lower-dimensional space.\n",
    "\n",
    "- The cumulative explained variance, which is the sum of variances explained by all the selected principal components, is often used to assess how much of the total variance is retained when reducing the dimensionality of the data. It helps determine how well the chosen components capture the data's overall variation.\n",
    "\n",
    "In summary, spread and variance in PCA are interconnected concepts. The spread of data along principal components reflects the distribution of data points in those directions, while the variance explained by each principal component quantifies the importance of capturing variation along that specific direction. PCA aims to find principal components that maximize the variance (spread) they explain, thereby retaining the most significant information in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b4e69-b415-45cb-b2e5-99bb7373f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How does PCA use the spread and variance of the data to identify principal components?\n",
    "Ans :\n",
    "    Principal Component Analysis (PCA) uses the spread and variance of the data to identify the principal components through an eigenvalue decomposition of the data's covariance matrix. Here's how the spread and variance are involved in the PCA process:\n",
    "\n",
    "1. **Data Standardization (Optional):** Before applying PCA, it is common practice to standardize the data, which involves centering the data (subtracting the mean) and scaling it (dividing by the standard deviation) to ensure that all features have the same scale. Standardization does not change the spread of the data but ensures that each feature contributes equally to the PCA.\n",
    "\n",
    "2. **Covariance Matrix Calculation:** PCA starts by calculating the covariance matrix of the data. The covariance matrix, often denoted as C, represents the relationships between pairs of features and how they vary together. Each element C[i][j] of the covariance matrix represents the covariance between feature i and feature j.\n",
    "\n",
    "3. **Eigenvalue Decomposition:** The core of PCA involves performing an eigenvalue decomposition (eigendecomposition) of the covariance matrix C. This decomposition yields a set of eigenvectors and corresponding eigenvalues.\n",
    "\n",
    "   - The eigenvectors represent the principal components of the data. Each eigenvector points in a direction in the original feature space, indicating how the data varies in that direction.\n",
    "\n",
    "   - The eigenvalues associated with each eigenvector quantify the amount of variance explained by that principal component. The larger the eigenvalue, the more variance the corresponding principal component captures.\n",
    "\n",
    "4. **Selection of Principal Components:** PCA ranks the eigenvalues in descending order. The principal components are selected based on the eigenvalues. The first principal component corresponds to the eigenvector with the largest eigenvalue, the second principal component corresponds to the eigenvector with the second-largest eigenvalue, and so on.\n",
    "\n",
    "   - Principal components that have larger eigenvalues explain more of the total variance in the data, indicating that they capture the primary directions of variation. These components are considered more important.\n",
    "\n",
    "   - You can choose a subset of the top-ranked principal components based on the amount of variance you want to retain or the dimensionality reduction you desire. The cumulative explained variance is often used to assess how many principal components are sufficient to retain a desired amount of variance.\n",
    "\n",
    "In summary, PCA identifies principal components by analyzing the spread and variance of the data. It calculates the covariance matrix to quantify how features vary together. The eigendecomposition of the covariance matrix reveals the principal components, where the eigenvalues indicate the amount of variance each component captures. By selecting principal components based on their associated eigenvalues, PCA provides a reduced-dimensional representation of the data while preserving as much of the original variance (spread) as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24c2af-d7c4-48f2-8ef0-4b183c33b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. How does PCA handle data with high variance in some dimensions but low variance in others?\n",
    "Ans : \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16952788-f4ac-4cbb-9cc3-e8f456fb80c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d09f6-06ae-47bf-8fa5-ba34b1ed5ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
