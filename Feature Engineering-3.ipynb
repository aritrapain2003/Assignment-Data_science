{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba095fe8-3e93-4225-ba13-a9ac5b060209",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "\n",
    "Answer: \n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numerical features within a specific range. It transforms the original feature values into a new range, typically between 0 and 1. Min-Max scaling is particularly useful when features have different scales, and you want to ensure that all features contribute equally to the analysis and modeling process.\n",
    "\n",
    "The Min-Max scaling formula for a feature x is given by:\n",
    "\n",
    "Scaled_value = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "where min(x) is the minimum value of the feature, max(x) is the maximum value, and Scaled_value is the transformed value of x within the [0, 1] range.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's say we have a dataset of house prices with two features: \"Area\" (ranging from 500 sq. ft to 3000 sq. ft) and \"Price\" (ranging from $100,000 to $1,000,000). Before applying Min-Max scaling, the dataset might look like this:\n",
    "\n",
    "\n",
    "Area    Price\n",
    "500     100,000\n",
    "1500    350,000\n",
    "2500    600,000\n",
    "3000    1,000,000\n",
    "\n",
    "\n",
    "To apply Min-Max scaling to the \"Area\" feature:\n",
    "\n",
    "1. Find the minimum and maximum values of the \"Area\" feature:\n",
    "   min(Area) = 500\n",
    "   max(Area) = 3000\n",
    "\n",
    "2. Apply the Min-Max scaling formula for each value of \"Area\":\n",
    "\n",
    "   Scaled_Area = (Area - min(Area)) / (max(Area) - min(Area))\n",
    "   Scaled_Area = (500 - 500) / (3000 - 500) = 0\n",
    "   Scaled_Area = (1500 - 500) / (3000 - 500) = 0.5\n",
    "   Scaled_Area = (2500 - 500) / (3000 - 500) = 1.0\n",
    "   Scaled_Area = (3000 - 500) / (3000 - 500) = 1.0\n",
    "\n",
    "The scaled \"Area\" values now range between 0 and 1:\n",
    "\n",
    "\n",
    "Scaled_Area\n",
    "0.0\n",
    "0.5\n",
    "1.0\n",
    "1.0\n",
    "\n",
    "\n",
    "Similarly, you can apply Min-Max scaling to the \"Price\" feature using the minimum and maximum values of the \"Price\" feature to transform the values between 0 and 1.\n",
    "\n",
    "By using Min-Max scaling, both \"Area\" and \"Price\" features are now in the same range, making them more comparable and suitable for analysis and modeling purposes. Min-Max scaling ensures that the features' original scales do not dominate the modeling process, leading to a more balanced and accurate representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49641ac-667e-4661-a6b6-db4b9cef6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "# Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "The Unit Vector technique, also known as Vector Normalization or L2 normalization, is a feature scaling method used to transform numerical features to have a unit norm. In this technique, each feature vector is divided by its Euclidean norm (L2 norm), which is the square root of the sum of the squares of its individual elements. The result is that all feature vectors have a length of 1 (i.e., a unit norm), and the direction of the vector is preserved.\n",
    "\n",
    "The formula for applying Unit Vector scaling to a feature vector x is:\n",
    "\n",
    "Scaled_vector = x / ||x||\n",
    "\n",
    "where ||x|| represents the L2 norm of the vector x.\n",
    "\n",
    "The Unit Vector technique is particularly useful when you want to scale features without worrying about their original ranges and when you are more interested in the direction of the vectors than their absolute magnitudes.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "\n",
    "The main difference between the Unit Vector technique and Min-Max scaling lies in how they transform the features:\n",
    "\n",
    "1. Min-Max Scaling: Min-Max scaling transforms features to a specific range, typically between 0 and 1. The scaling is performed using the minimum and maximum values of each feature. It ensures that all features are on the same scale, but it does not change the direction or orientation of the data.\n",
    "\n",
    "2. Unit Vector Scaling: Unit Vector scaling normalizes features to have a unit norm, meaning all feature vectors have a length of 1. It preserves the direction of the data while scaling all vectors to the same magnitude. It is useful when you want to consider the relative directions of the vectors and when the magnitudes of the features are not as critical.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with two numerical features: \"Age\" and \"Income.\" Let's assume the dataset looks like this:\n",
    "\n",
    "\n",
    "Age   Income\n",
    "25    50000\n",
    "35    75000\n",
    "45    100000\n",
    "\n",
    "\n",
    "To apply the Unit Vector technique:\n",
    "\n",
    "1. Compute the L2 norm for each feature vector:\n",
    "   - For the first row (25, 50000), L2 norm = √(25^2 + 50000^2) ≈ 50000.25\n",
    "   - For the second row (35, 75000), L2 norm = √(35^2 + 75000^2) ≈ 75000.21\n",
    "   - For the third row (45, 100000), L2 norm = √(45^2 + 100000^2) ≈ 100000.37\n",
    "\n",
    "2. Divide each feature vector by its L2 norm to obtain the scaled vector:\n",
    "   - For the first row, Scaled_vector = (25, 50000) / 50000.25 ≈ (0.0005, 0.99999)\n",
    "   - For the second row, Scaled_vector = (35, 75000) / 75000.21 ≈ (0.000467, 0.99999)\n",
    "   - For the third row, Scaled_vector = (45, 100000) / 100000.37 ≈ (0.00045, 0.99999)\n",
    "\n",
    "After applying the Unit Vector technique, all feature vectors have a length of approximately 1, preserving their directions while eliminating the differences in their magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28aa4a-948c-4da6-a8e3-e58fae6763e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "# example to illustrate its application.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PCA (Principal Component Analysis) is a widely used dimensionality reduction technique in machine learning and data analysis. It aims to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important patterns and variations in the data. PCA achieves this by finding the principal components, which are new orthogonal (uncorrelated) axes in the data space, aligned with the directions of maximum variance.\n",
    "\n",
    "The key steps of PCA are as follows:\n",
    "\n",
    "1. Data Centering: Center the data by subtracting the mean of each feature from its corresponding data points. This step ensures that the data is centered around the origin.\n",
    "\n",
    "2. Covariance Matrix: Compute the covariance matrix of the centered data. The covariance matrix shows the relationships between different features and their variations.\n",
    "\n",
    "3. Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix. The eigenvalue decomposition provides the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "4. Principal Components: Sort the eigenvalues in descending order and choose the corresponding eigenvectors as the principal components. The principal components represent the new axes in the lower-dimensional space.\n",
    "\n",
    "5. Dimensionality Reduction: Project the original data onto the selected principal components to obtain the lower-dimensional representation of the data.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset of two numerical features, \"Height\" (in cm) and \"Weight\" (in kg), representing information about individuals. The dataset might look like this:\n",
    "\n",
    "\n",
    "Height  Weight\n",
    "180     75\n",
    "165     60\n",
    "175     68\n",
    "155     50\n",
    "\n",
    "\n",
    "To apply PCA for dimensionality reduction:\n",
    "\n",
    "1. Data Centering: Calculate the mean of each feature and subtract it from the corresponding data points to center the data. The mean of \"Height\" is (180 + 165 + 175 + 155) / 4 = 168.75, and the mean of \"Weight\" is (75 + 60 + 68 + 50) / 4 = 63.25.\n",
    "\n",
    "\n",
    "Height  Weight\n",
    "11.25   11.75\n",
    "-3.75   -3.25\n",
    "6.25    4.75\n",
    "-13.75  -13.25\n",
    "\n",
    "\n",
    "2. Covariance Matrix: Compute the covariance matrix of the centered data.\n",
    "\n",
    "\n",
    "Covariance Matrix:\n",
    "      Height   Weight\n",
    "Height  75.00    45.00\n",
    "Weight  45.00    27.67\n",
    "\n",
    "\n",
    "3. Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix.\n",
    "\n",
    "\n",
    "Eigenvalues: [ 99.483   2.187]\n",
    "Eigenvectors:\n",
    "     [ 0.850   0.527]\n",
    "     [-0.527   0.850]\n",
    "\n",
    "\n",
    "4. Principal Components: The eigenvalues represent the amount of variance explained by each principal component. The first principal component (eigenvector [0.850, 0.527]) explains the majority of the variance, while the second principal component (eigenvector [-0.527, 0.850]) explains the remaining variance.\n",
    "\n",
    "5. Dimensionality Reduction: Project the original data onto the selected principal components (in this case, only the first principal component) to obtain the lower-dimensional representation.\n",
    "\n",
    "\n",
    "Lower-dimensional representation:\n",
    "11.75\n",
    "-3.25\n",
    "4.75\n",
    "-13.25\n",
    "\n",
    "\n",
    "After applying PCA, the data is now represented in a one-dimensional space (reduced from two dimensions), which captures most of the variance in the original dataset. This lower-dimensional representation can be used for various purposes, such as visualization, clustering, or building machine learning models, with reduced computational complexity and potential improved performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab8693-0d4b-46f8-b607-f6d5d9475aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "\n",
    "\n",
    "PCA and Feature Extraction are closely related concepts, and PCA can be used as a feature extraction technique. Both PCA and feature extraction aim to reduce the dimensionality of the data by transforming the original features into a new set of features (also known as components or factors). The key difference between the two lies in the purpose of the transformation:\n",
    "\n",
    "- PCA: Principal Component Analysis is a specific technique for dimensionality reduction that focuses on preserving the maximum variance in the data. It identifies the directions (principal components) along which the data varies the most. The principal components are orthogonal and ranked in order of the amount of variance they capture, allowing the reduction of dimensions while retaining the most important patterns and variations in the data.\n",
    "\n",
    "- Feature Extraction: Feature extraction is a broader term that encompasses various techniques used to create new features from the original features while preserving or enhancing specific characteristics of the data. The extracted features are typically engineered to capture certain relevant information or patterns in the data, which may not be directly captured by the original features. Feature extraction can be used for simplifying complex datasets, enhancing the data's separability, and improving the performance of machine learning algorithms.\n",
    "\n",
    "The relationship between PCA and Feature Extraction is that PCA can be considered a type of feature extraction technique, specifically focused on capturing variance as the most important characteristic. PCA extracts the features that represent the directions of maximum variance in the data.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset of four numerical features, representing different properties of houses: \"Size\" (in square feet), \"Age\" (in years), \"Number of Rooms,\" and \"Price\" (in dollars). The dataset might look like this:\n",
    "\n",
    "```\n",
    "Size   Age   Rooms   Price\n",
    "2000   10    3       300000\n",
    "1500   5     2       200000\n",
    "2500   15    4       400000\n",
    "1800   8     3       280000\n",
    "```\n",
    "\n",
    "To use PCA for feature extraction:\n",
    "\n",
    "1. Data Preprocessing: Begin by standardizing the data by subtracting the mean and dividing by the standard deviation for each feature. This step ensures that all features are on the same scale.\n",
    "\n",
    "2. PCA: Apply PCA to the preprocessed data to extract the principal components. The number of principal components to retain is a hyperparameter that can be determined based on the amount of variance explained or domain knowledge.\n",
    "\n",
    "3. Feature Extraction: The extracted principal components serve as the new set of features, replacing the original features. Each principal component is a linear combination of the original features, capturing different patterns and variations in the data.\n",
    "\n",
    "Suppose PCA is used to retain two principal components, which capture 95% of the total variance:\n",
    "\n",
    "```\n",
    "Principal Component 1:  [0.7, -0.7, 0.58, 0.12]\n",
    "Principal Component 2:  [-0.28, 0.28, 0.74, -0.58]\n",
    "```\n",
    "\n",
    "The original four features are now transformed into two principal components. These principal components represent the most important patterns in the data, capturing the directions of maximum variance. You can use these two principal components as new features for further analysis or modeling, effectively reducing the dimensionality of the dataset from four to two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59902b-6db7-4bd9-af5a-87e1637d2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "1. Data Understanding: Understand the dataset and the characteristics of each feature. Identify the features that need to be scaled.\n",
    "\n",
    "2. Data Extraction: Extract the relevant features from the dataset, such as \"price,\" \"rating,\" and \"delivery time.\"\n",
    "\n",
    "3. Min-Max Scaling: Apply Min-Max scaling to the selected features. Min-Max scaling transforms the values of each feature to a range between 0 and 1, ensuring that all features have the same scale.\n",
    "\n",
    "The formula for Min-Max scaling of a feature x is given by:\n",
    "\n",
    "Scaled_value = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "where min(x) is the minimum value of the feature, max(x) is the maximum value, and Scaled_value is the transformed value of x within the [0, 1] range.\n",
    "\n",
    "4. Example:\n",
    "\n",
    "Let's consider a small subset of the food delivery service dataset with three features: \"price,\" \"rating,\" and \"delivery time.\" The original data might look like this:\n",
    "\n",
    "```\n",
    "Price   Rating   Delivery Time\n",
    "$10     4.5      30 minutes\n",
    "$15     4.0      45 minutes\n",
    "$8      3.8      25 minutes\n",
    "$12     4.2      35 minutes\n",
    "```\n",
    "\n",
    "To apply Min-Max scaling to the \"price\" feature:\n",
    "\n",
    "1. Find the minimum and maximum values of the \"price\" feature:\n",
    "   min(Price) = $8\n",
    "   max(Price) = $15\n",
    "\n",
    "2. Apply the Min-Max scaling formula for each value of \"price\":\n",
    "\n",
    "   Scaled_price = (Price - min(Price)) / (max(Price) - min(Price))\n",
    "   Scaled_price = ($10 - $8) / ($15 - $8) ≈ 0.25\n",
    "   Scaled_price = ($15 - $8) / ($15 - $8) = 1.00\n",
    "   Scaled_price = ($8 - $8) / ($15 - $8) = 0.00\n",
    "   Scaled_price = ($12 - $8) / ($15 - $8) ≈ 0.75\n",
    "\n",
    "The scaled \"price\" values now range between 0 and 1:\n",
    "\n",
    "```\n",
    "Scaled Price\n",
    "0.25\n",
    "1.00\n",
    "0.00\n",
    "0.75\n",
    "```\n",
    "\n",
    "You would follow a similar process to apply Min-Max scaling to the \"rating\" and \"delivery time\" features, ensuring that all features are transformed to the [0, 1] range. After preprocessing the data using Min-Max scaling, all features will have the same scale, which is essential for building a recommendation system that considers the relative importance of different features equally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33488fd7-1c65-4280-a521-dfd18bc8d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To reduce the dimensionality of the dataset containing many features for predicting stock prices, you can use PCA (Principal Component Analysis). PCA will help identify the most important patterns and variations in the data and create a lower-dimensional representation of the original features. Here's how you can use PCA for dimensionality reduction in the context of building a stock price prediction model:\n",
    "\n",
    "1. Data Preprocessing: Begin by preparing the dataset, handling missing values, encoding categorical variables, and standardizing the numerical features (subtract the mean and divide by the standard deviation for each feature) to ensure all features are on the same scale.\n",
    "\n",
    "2. Apply PCA: Use PCA to transform the standardized dataset into a lower-dimensional representation. The PCA algorithm will calculate the principal components, which are new orthogonal axes in the data space, representing the directions of maximum variance.\n",
    "\n",
    "3. Determine the Number of Principal Components: Decide on the number of principal components to retain. You can choose based on the amount of variance you want to explain (e.g., retain components explaining a certain percentage of the total variance) or based on domain knowledge.\n",
    "\n",
    "4. Feature Reduction: Select the top principal components that explain the most variance in the data. These components capture the most significant patterns and trends in the original dataset.\n",
    "\n",
    "5. Project Data: Project the original data onto the selected principal components to obtain the lower-dimensional representation of the data.\n",
    "\n",
    "6. Stock Price Prediction Model: Use the reduced dataset with fewer dimensions as input to train your stock price prediction model. You can apply various machine learning algorithms such as regression, time series forecasting models, or deep learning techniques for stock price prediction.\n",
    "\n",
    "Benefits of using PCA for Dimensionality Reduction in Stock Price Prediction:\n",
    "\n",
    "- Enhanced Efficiency: By reducing the number of features, the computational complexity and memory requirements of your stock price prediction model decrease, resulting in faster training and inference times.\n",
    "\n",
    "- Noise Reduction: PCA can help remove noise and irrelevant information present in the dataset, making the model more robust and generalizable.\n",
    "\n",
    "- Feature Selection: PCA implicitly performs feature selection by focusing on the principal components with the most variance, which often correspond to the most relevant and informative features.\n",
    "\n",
    "- Improved Model Performance: A lower-dimensional dataset obtained from PCA can lead to improved model performance and avoid overfitting, especially when dealing with high-dimensional datasets.\n",
    "\n",
    "However, it's crucial to note that while PCA is a powerful technique for dimensionality reduction, it may not always be the best choice for every dataset or prediction problem. It's essential to consider the trade-offs, interpretability of results, and the specific characteristics of your data and prediction task when deciding whether to apply PCA for dimensionality reduction in the stock price prediction project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0eb56-2070-4821-a6e9-397a203f7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "# values to a range of -1 to 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To perform Min-Max scaling to transform the values of the dataset [1, 5, 10, 15, 20] to a range of -1 to 1, follow these steps:\n",
    "\n",
    "1. Find the minimum and maximum values of the dataset:\n",
    "   min = 1\n",
    "   max = 20\n",
    "\n",
    "2. Apply the Min-Max scaling formula for each value in the dataset:\n",
    "\n",
    "   Scaled_value = (value - min) / (max - min)\n",
    "\n",
    "   Scaled_value(1) = (1 - 1) / (20 - 1) = 0 / 19 ≈ 0\n",
    "   Scaled_value(5) = (5 - 1) / (20 - 1) = 4 / 19 ≈ 0.21\n",
    "   Scaled_value(10) = (10 - 1) / (20 - 1) = 9 / 19 ≈ 0.47\n",
    "   Scaled_value(15) = (15 - 1) / (20 - 1) = 14 / 19 ≈ 0.74\n",
    "   Scaled_value(20) = (20 - 1) / (20 - 1) = 19 / 19 = 1\n",
    "\n",
    "3. Scale the values to the desired range of -1 to 1:\n",
    "   Scaled_value(-1) = (2 * Scaled_value) - 1\n",
    "\n",
    "   Scaled_value(-1) = (2 * 0) - 1 = -1\n",
    "   Scaled_value(-0.21) = (2 * 0.21) - 1 ≈ -0.58\n",
    "   Scaled_value(0.47) = (2 * 0.47) - 1 ≈ -0.06\n",
    "   Scaled_value(0.74) = (2 * 0.74) - 1 ≈ 0.47\n",
    "   Scaled_value(1) = (2 * 1) - 1 = 1\n",
    "\n",
    "The Min-Max scaled values of the dataset [1, 5, 10, 15, 20] transformed to a range of -1 to 1 are approximately:\n",
    "\n",
    "[-1, -0.58, -0.06, 0.47, 1]\n",
    "\n",
    "Now, all the values are within the desired range of -1 to 1, preserving the relative relationships among the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a4d06-cf5e-4b04-bd95-679a19925196",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "\n",
    "A\n",
    "\n",
    "\n",
    "To perform Feature Extraction using PCA on the dataset with features [height, weight, age, gender, blood pressure], we need to apply the PCA algorithm to find the principal components. The number of principal components to retain depends on the amount of variance we want to explain and the trade-off between the reduced dimensionality and the loss of information.\n",
    "\n",
    "Here are the steps to perform Feature Extraction using PCA:\n",
    "\n",
    "1. Data Preprocessing: Before applying PCA, it is essential to preprocess the data, including handling missing values, encoding categorical variables (e.g., gender), and standardizing the numerical features (height, weight, age, blood pressure) to ensure they are on the same scale.\n",
    "\n",
    "2. PCA Application: Apply PCA to the preprocessed data. PCA will calculate the principal components, which are new orthogonal axes in the data space, representing the directions of maximum variance.\n",
    "\n",
    "3. Eigenvalues and Explained Variance: Examine the eigenvalues obtained during the PCA. Eigenvalues represent the amount of variance explained by each principal component. The total variance in the data is the sum of all eigenvalues. You can calculate the explained variance ratio for each principal component by dividing the eigenvalue of each component by the total variance.\n",
    "\n",
    "4. Determine the Number of Principal Components: Decide on the number of principal components to retain based on the amount of variance you want to explain. A common approach is to choose the smallest number of components that cumulatively explain a significant percentage of the total variance, such as 90% or 95%.\n",
    "\n",
    "5. Feature Reduction: Select the top principal components that explain the desired percentage of variance in the data. These components capture the most significant patterns and variations in the original dataset.\n",
    "\n",
    "6. Project Data: Project the original data onto the selected principal components to obtain the lower-dimensional representation of the data.\n",
    "\n",
    "The choice of the number of principal components to retain is a critical decision in PCA. Retaining too few components may result in losing important information, while retaining too many components may not provide a significant reduction in dimensionality.\n",
    "\n",
    "The optimal number of principal components to retain depends on the specific dataset, the application, and the trade-off between model complexity and performance. As a rule of thumb, you can choose to retain enough principal components to explain a high percentage of the total variance, such as 90% or more.\n",
    "\n",
    "For example, if the first three principal components explain 95% of the total variance in the data, you may decide to retain these three components for dimensionality reduction. Retaining three components would mean reducing the dataset's dimensionality from five features (height, weight, age, gender, blood pressure) to three principal components, capturing the most important patterns in the data while reducing its complexity.\n",
    "\n",
    "Ultimately, the number of principal components to retain should be determined through experimentation, cross-validation, and considering the specific requirements and constraints of the stock price prediction project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10452135-b5e1-41cf-9ca8-fb80d84f8566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bdc3b-1932-41cc-994e-4e9eaad5ee14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777e345-35b6-4036-b68e-817b397d0fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f499d-15d3-4e4c-8067-9b27f1434581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
